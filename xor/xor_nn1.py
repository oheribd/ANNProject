import numpy as np
from unagi import Chan

def sigmoid(z):
  return 1/(1 + np.exp(-z))

def tanh(z):
  return np.tanh(z)

  def yon(self,g):
    return g*(1-self.h**2)  

def ReLU(z):
  return abs(z) * (z>0)
        

def initialize_parameters(n_x, n_h1,n_h2, n_y):
  W1 = np.random.randn(n_h1, n_x)
  b1 = np.zeros((n_h1, 1))
  W2 = np.random.randn(n_h2, n_h1)
  b2 = np.zeros((n_h2, 1))
  W3 = np.random.randn(n_y, n_h2)
  b3 = np.zeros((n_y, 1))

  parameters = {
    "W1": W1,
    "b1" : b1,
    "W2": W2,
    "b2" : b2,
    "W3": W3,
    "b3" : b3
  }
  return parameters

def forward_prop(X, parameters):
  W1 = parameters["W1"]
  b1 = parameters["b1"]
  W2 = parameters["W2"]
  b2 = parameters["b2"]
  W3 = parameters["W3"]
  b3 = parameters["b3"]

  Z1 = np.dot(W1, X) + b1
  #hidden use tanh
  A1 = tanh(Z1)
  Z2 = np.dot(W2, A1) + b2
  #output use sigmoid
  A2 = tanh(Z2)
  Z3 = np.dot(W3, A2) + b3
  A3 = tanh(Z3)

  cache = {
    "A1": A1,
    "A2": A2,
    "A3": A3
  }
  return A3, cache

def backward_prop(X, Y, cache, parameters):
  A1 = cache["A1"]
  A2 = cache["A2"]
  A3 = cache["A3"]

  W3 = parameters["W3"]


  dZ3 = A3 - Y
  dW3 = np.dot(dZ3, A2.T)/m
  db3 = np.sum(dZ3, axis=1, keepdims=True)/m
  dZ2 = A2 - Y
  dW2 = np.dot(dZ2, A1.T)/m
  db2 = np.sum(dZ2, axis=1, keepdims=True)/m
  dZ1 = np.multiply(np.dot(W3.T, dZ2), 1-np.power(A1, 2))
  dW1 = np.dot(dZ1, X.T)/m
  db1 = np.sum(dZ1, axis=1, keepdims=True)/m

  grads = {
    "dW1": dW1,
    "db1": db1,
    "dW2": dW2,
    "db2": db2,
    "dW3": dW3,
    "db3": db3
  }

  return grads

def update_parameters(parameters, grads, learning_rate):
  W1 = parameters["W1"]
  b1 = parameters["b1"]
  W2 = parameters["W2"]
  b2 = parameters["b2"]
  W3 = parameters["W3"]
  b3 = parameters["b3"]


  dW1 = grads["dW1"]
  db1 = grads["db1"]
  dW2 = grads["dW2"]
  db2 = grads["db2"]
  dW3 = grads["dW3"]
  db3 = grads["db3"]


  W1 = W1 - learning_rate*dW1
  b1 = b1 - learning_rate*db1
  W2 = W2 - learning_rate*dW2
  b2 = b2 - learning_rate*db2
  W3 = W3 - learning_rate*dW3
  b3 = b3 - learning_rate*db3
  
  
  new_parameters = {
    "W1": W1,
    "W2": W2,
    "b1" : b1,
    "b2" : b2,
    "W3": W3,
    "b3" : b3
  }

  return new_parameters

def model(X, Y, n_x, n_h1, n_h2, n_y, num_of_iters, learning_rate):
  parameters = initialize_parameters(n_x, n_h1, n_h2, n_y)

  for i in range(0, num_of_iters+1):
    a2, cache = forward_prop(X, parameters)

    grads = backward_prop(X, Y, cache, parameters)

    parameters = update_parameters(parameters, grads, learning_rate)

    if(i%100 == 0):
      print('Cost after iteration# {:d}: {:f}'.format(i, cost))

  return parameters

def predict(X, parameters):
  a2, cache = forward_prop(X, parameters)
  yhat = a2
  yhat = np.squeeze(yhat)
  print(yhat)
  if(yhat >= 0.541):
    y_predict = 1
  else:
    y_predict = 0

  return y_predict

np.random.seed(2)

# The 4 training examples by columns
X = np.array([[0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1
 ], [0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.82,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.8,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.77,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.8,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.82,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.87,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.93,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.95,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.98,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,1,0,0.01,0.04,0.07,0.11,0.15,0.19,0.22,0.23,0.26,0.3,0.32,0.38,0.44,0.46,0.49,0.52,0.55,0.56,0.62,0.64,0.67,0.71,0.73,0.77,0.8,0.82,0.87,0.93,0.95,0.98
]])

# The outputs of the XOR for every example in X
Y = np.array([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
]])

# No. of training examples
m = X.shape[1]
# Set the hyperparameters
n_x = 2     #No. of neurons in first layer
n_h1 = 32     #No. of neurons in hidden layer
n_h2 = 32
n_y = 1     #No. of neurons in output layer
num_of_iters = 3000
learning_rate = 0.5

trained_parameters = model(X, Y, n_x, n_h1, n_h2, n_y, num_of_iters, learning_rate)

# Test 2X1 vector to calculate the XOR of its elements. 
# You can try any of those: (0, 0), (0, 1), (1, 0), (1, 1)
X_test = np.array([[0.99], [0.02]])
y_predict = predict(X_test, trained_parameters)
# Print the result
print('Neural Network prediction for example ({:f}, {:f}) is {:f}'.format(
    X_test[0][0], X_test[1][0], y_predict))